{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EV03SjaAHs3T"
      },
      "outputs": [],
      "source": [
        "# NER Modeling with spaCy and NLTK\n",
        "\n",
        "## Step 1: Setup and Installation\n",
        "!pip install nltk spacy matplotlib pandas\n",
        "!python3 -m spacy download en_core_web_sm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import spacy\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "WqlHjwreHvCd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load spaCy Model\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('maxent_ne_chunker_tab')\n",
        "nltk.download('words')\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VdR0byf9Hxwi",
        "outputId": "70d9bdbe-5abd-4687-f8cf-70c17e7e3395"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 2: Named Entity Recognition with spaCy\n",
        "sample_text = \"Barack Obama was born in Hawaii. He was the 44th President of the United States. Apple Inc. is a technology company headquartered in Cupertino.\"\n",
        "\n",
        "print(\"\\nProcessing Text with spaCy:\")\n",
        "doc = nlp(sample_text)\n",
        "\n",
        "print(\"\\nExtracted Named Entities:\")\n",
        "for ent in doc.ents:\n",
        "    print(f\"{ent.text} ({ent.label_})\")\n",
        "\n",
        "from spacy import displacy\n",
        "displacy.render(doc, style=\"ent\", jupyter=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "dF6305U5H1zS",
        "outputId": "3b015d63-7307-4ec3-857c-8b68ce7dd2b3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing Text with spaCy:\n",
            "\n",
            "Extracted Named Entities:\n",
            "Barack Obama (PERSON)\n",
            "Hawaii (GPE)\n",
            "44th (ORDINAL)\n",
            "the United States (GPE)\n",
            "Apple Inc. (ORG)\n",
            "Cupertino (GPE)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Barack Obama\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " was born in \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Hawaii\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              ". He was the \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    44th\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORDINAL</span>\n",
              "</mark>\n",
              " President of \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    the United States\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              ". \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Apple Inc.\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " is a technology company headquartered in \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Cupertino\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              ".</div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 3: NER with NLTK\n",
        "# NLTK does not have a built-in NER model but provides a pre-trained model\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "from nltk.tree import Tree\n",
        "\n",
        "# Tokenize and Tag\n",
        "nltk_tokens = word_tokenize(sample_text)\n",
        "pos_tags = pos_tag(nltk_tokens)\n",
        "\n",
        "# Named Entity Chunking\n",
        "print(\"\\nNamed Entity Chunking with NLTK:\")\n",
        "ne_tree = ne_chunk(pos_tags)\n",
        "for subtree in ne_tree:\n",
        "    if isinstance(subtree, Tree):\n",
        "        entity_name = \" \".join([token for token, pos in subtree.leaves()])\n",
        "        entity_type = subtree.label()\n",
        "        print(f\"{entity_name} ({entity_type})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sui6UWGTH4aV",
        "outputId": "302dbb2a-4933-41cc-a914-2b28dbf37d4f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Named Entity Chunking with NLTK:\n",
            "Barack (PERSON)\n",
            "Obama (PERSON)\n",
            "Hawaii (GPE)\n",
            "United States (GPE)\n",
            "Apple Inc. (PERSON)\n",
            "Cupertino (GPE)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 4: Custom NER Training with spaCy\n",
        "# Task 1: Prepare training data and train a custom NER model\n",
        "# Enhanced Training Function for Custom NER Model\n",
        "def train_custom_ner_model(train_data):\n",
        "    from spacy.training.example import Example\n",
        "    from spacy.util import minibatch\n",
        "\n",
        "    ner_model = spacy.blank(\"en\")  # Create a blank model\n",
        "    ner = ner_model.add_pipe(\"ner\", last=True)  # Add NER pipe\n",
        "    '''\n",
        "    Insert your code here\n",
        "    # Add entity labels\n",
        "\n",
        "    # Start training\n",
        "\n",
        "    '''\n",
        "    return ner_model\n",
        "\n",
        "TRAIN_DATA = [\n",
        "    (\"Google is a tech company in Mountain View.\", {\"entities\": [(0, 6, \"ORG\"), (29, 42, \"GPE\")]}),\n",
        "    (\"Elon Musk is the CEO of SpaceX.\", {\"entities\": [(0, 9, \"PERSON\"), (26, 32, \"ORG\")]}),\n",
        "    (\"The Eiffel Tower is in Paris.\", {\"entities\": [(4, 17, \"LOC\"), (23, 28, \"GPE\")]}),\n",
        "    (\"Tesla's headquarters is in Palo Alto.\", {\"entities\": [(0, 5, \"ORG\"), (27, 36, \"GPE\")]}),\n",
        "    (\"Amazon is hiring engineers in Seattle.\", {\"entities\": [(0, 6, \"ORG\"), (33, 40, \"GPE\")]}),\n",
        "]\n",
        "\n",
        "print(\"\\nTraining Custom NER Model:\")\n",
        "custom_ner_model = train_custom_ner_model(TRAIN_DATA)\n",
        "\n",
        "custom_ner_model.to_disk(\"custom_ner_model\")\n",
        "print(\"Custom NER model saved.\")\n",
        "\n",
        "loaded_model = spacy.load(\"custom_ner_model\")\n",
        "\n",
        "def test_custom_ner_model(model, test_texts):\n",
        "    results = {}\n",
        "    for text in test_texts:\n",
        "        doc = model(text)\n",
        "        results[text] = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "    return results\n",
        "\n",
        "test_sentences = [\n",
        "    \"Tesla is building new factories in Texas.\",\n",
        "    \"Elon Musk is visiting Berlin.\",\n",
        "    \"Amazon's revenue grew in Seattle.\",\n",
        "    \"The Eiffel Tower is a popular tourist spot.\",\n",
        "]\n",
        "\n",
        "print(\"\\nTesting Custom NER Model:\")\n",
        "test_results = test_custom_ner_model(loaded_model, test_sentences)\n",
        "for sentence, entities in test_results.items():\n",
        "    print(f\"Sentence: {sentence}\")\n",
        "    print(f\"Extracted Entities: {entities}\")\n"
      ],
      "metadata": {
        "id": "t1uO1y0sH6wg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 5: Text Preprocessing for Sentiment Analysis\n",
        "# Task 2: Write functions for preprocessing textby lemmatizing and removing stopwords\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    doc = nlp(text)\n",
        "    '''Insert your code here'''\n",
        "    tokens = []\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "text_data = [\n",
        "    \"I love programming in Python!\",\n",
        "    \"The weather today is terrible.\",\n",
        "    \"I had an amazing time at the concert last night!\",\n",
        "    \"The movie was not good at all.\",\n",
        "]\n",
        "\n",
        "preprocessed_texts = [preprocess_text(text) for text in text_data]\n",
        "print(\"\\nPreprocessed Texts:\")\n",
        "print(preprocessed_texts)"
      ],
      "metadata": {
        "id": "j3OGOy7dH-3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 6: Word Frequency Analysis\n",
        "# Task 3: Analyze word frequency and plot results\n",
        "def analyze_word_frequency(texts):\n",
        "    '''Complete this function'''\n",
        "    return word_freq\n",
        "\n",
        "word_freq = analyze_word_frequency(preprocessed_texts)\n",
        "print(\"\\nWord Frequency Analysis:\")\n",
        "print(word_freq.head())\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "word_freq.head(10).plot(kind=\"bar\", color=\"skyblue\")\n",
        "plt.title(\"Top 10 Frequent Words\")\n",
        "plt.xlabel(\"Words\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "palbZP8oIDgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 7: Preparing for Twitter Sentiment Analysis\n",
        "# Task 4: Write a function to clean tweets by removing mentions, links, special characters, lowercase and strips\n",
        "import re\n",
        "\n",
        "def clean_tweet(tweet):\n",
        "    '''Complete this function'''\n",
        "    return tweet\n",
        "\n",
        "sample_tweets = [\n",
        "    \"@user I love the new design! Check it out: https://example.com\",\n",
        "    \"This is the worst product ever. Total waste of money! #fail\",\n",
        "    \"Great job by the team on the latest release! #innovation\",\n",
        "]\n",
        "\n",
        "cleaned_tweets = [clean_tweet(tweet) for tweet in sample_tweets]\n",
        "print(\"\\nCleaned Tweets:\")\n",
        "print(cleaned_tweets)\n"
      ],
      "metadata": {
        "id": "kWY0cYiJIF0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Extract and analyze bigrams (pairs of words) and trigrams (triplets of words) from the cleaned tweets to find common patterns or phrases.\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "def generate_word_cloud(texts, title):\n",
        "    combined_text = \" \".join(texts)\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate(combined_text)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "    plt.title(title)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "generate_word_cloud(cleaned_tweets, \"Word Cloud for Cleaned Tweets\")\n"
      ],
      "metadata": {
        "id": "2yvGuCtNQSn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#step 9\n",
        "# Task 6: Use a basic scoring function based on the presence of positive and negative words and print whether cleaned tweets are positive or negative\n",
        "positive_words = {\"love\", \"great\", \"amazing\", \"awesome\", \"good\"}\n",
        "negative_words = {\"worst\", \"bad\", \"terrible\", \"fail\", \"waste\"}\n",
        "\n",
        "def simple_sentiment_score(tweet):\n",
        "    '''Complete the function'''\n",
        "    return\n",
        "\n",
        "# Assign sentiment scores to cleaned tweets\n",
        "sentiment_scores = [simple_sentiment_score(tweet) for tweet in cleaned_tweets]\n",
        "print(\"\\nSentiment Scores:\")\n",
        "print(sentiment_scores)\n"
      ],
      "metadata": {
        "id": "2vN8uoq_Qdys"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}